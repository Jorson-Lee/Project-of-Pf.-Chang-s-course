{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled0.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOCXlxM3/7dzIRNG4gjGmpf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"6J4skrBPr85L","executionInfo":{"status":"ok","timestamp":1640051518464,"user_tz":-480,"elapsed":309,"user":{"displayName":"AP28_许儒怡","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17244627543529180816"}}},"outputs":[],"source":["import cv2\n","import numpy as np\n","\n","CASCADE_PATH = \"haarcascade_frontalface_default.xml\"\n","\n","RESIZE_SCALE = 3\n","REC_COLOR = (0, 255, 0)\n","\n","def getFaceCoordinates(image):\n","    cascade = cv2.CascadeClassifier(CASCADE_PATH)\n","    \n","    img_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n","    img_gray = cv2.equalizeHist(img_gray)\n","    rects = cascade.detectMultiScale(\n","        img_gray,\n","        scaleFactor=1.1,\n","        minNeighbors=3,\n","        minSize=(48, 48)\n","        )\n","\n","    # For now, we only deal with the case that we detect one face.\n","    if(len(rects) != 1) :\n","        return None\n","    \n","    face = rects[0]\n","    bounding_box = [face[0], face[1], face[0] + face[2], face[1] + face[3]]\n","\n","    # return map((lambda x: x), bounding_box)\n","    return bounding_box\n","\n","def drawFace(img, faceCoordinates):\n","    cv2.rectangle(np.asarray(img), (faceCoordinates[0], faceCoordinates[1]), \\\n","    (faceCoordinates[2], faceCoordinates[3]), REC_COLOR, thickness=2)\n","\n","def crop_face(img, faceCoordinates):\n","    '''\n","    extend_len_x =  (256 - (faceCoordinates[3] - faceCoordinates[1]))/2\n","    extend_len_y =  (256 - (faceCoordinates[0] - faceCoordinates[2]))/2\n","    img_size = img.shape\n","    if (faceCoordinates[1] - extend_len_x) >= 0 :\n","        faceCoordinates[1] -= extend_len_x\n","    if (faceCoordinates[3] + extend_len_x) < img_size[0]:\n","        faceCoordinates[3] += extend_len_x\n","    if (faceCoordinates[0] - extend_len_y) >= 0 :\n","        faceCoordinates[0] -= extend_len_y\n","    if (faceCoordinates[2] + extend_len_y) < img_size[1]:\n","        faceCoordinates[2] += extend_len_y\n","    '''\n","    return img[faceCoordinates[1]:faceCoordinates[3], faceCoordinates[0]:faceCoordinates[2]]\n","\n","def preprocess(img, faceCoordinates, face_shape=(48, 48)):\n","    '''\n","        This function will crop user's face from the original frame\n","    '''\n","    face = crop_face(img, faceCoordinates)\n","    #face = img\n","    face_scaled = cv2.resize(face, face_shape)\n","    face_gray = cv2.cvtColor(face_scaled, cv2.COLOR_BGR2GRAY)\n","    \n","    return face_gray"]},{"cell_type":"code","source":["import argparse\n","import sys, os\n","sys.path.append(\"../\")\n","\n","import cv2\n","import numpy as np\n","\n","import face_detection_utilities as fdu\n","\n","import model.myVGG as vgg\n","import speech_recognition as sr\n","\n","windowsName = 'Preview Screen'\n","\n","parser = argparse.ArgumentParser(description='A live emotion recognition from webcam')\n","\n","args = parser.parse_args()\n","FACE_SHAPE = (48, 48)\n","\n","model = vgg.VGG_16('my_model_weights_83.h5')\n","#model = vgg.VGG_16()\n","\n","emo     = ['Angry', 'Fear', 'Happy',\n","           'Sad', 'Surprise', 'Neutral']\n","\n","def speechRecognition():\n","    # obtain audio from the microphone \n","    print(\"Press 'y' to start~\")\n","    inputdata = input()\n","    if inputdata == 'y':\n","        inputdata = 0\n","        r = sr.Recognizer()\n","        with sr.Microphone() as source:\n","            print(\"Say something!\")\n","            audio = r.listen(source)\n","        # recognize speech using Google Speech Recognition\n","        try:\n","        # for testing purposes, we're just using the default API key\n","        # to use another API key, use `r.recognize_google(audio, key=\"GOOGLE_SPEECH_RECOGNITION_API_KEY\")`\n","        # instead of `r.recognize_google(audio)`\n","            recSuccess = 1\n","            recContent = r.recognize_google(audio)\n","            print(\"Speech Recognition thinks you said \" + recContent)#,language=\"cmn-Hant-TW\")\n","            return recContent\n","        except sr.UnknownValueError:\n","            print(\"Could not understand audio\")\n","        except sr.RequestError as e:\n","            print(\"Could not request results from Google Speech Recognition service; {0}\".format(e))\n","\n","def refreshFrame(frame, faceCoordinates):\n","    if faceCoordinates is not None:\n","        fdu.drawFace(frame, faceCoordinates)\n","    cv2.imshow(windowsName, frame)\n","\n","\n","def showScreenAndDectect(capture):\n","    print(\"Face dececting...\")\n","    cnt = 5;\n","    while cnt:\n","        flag, frame = capture.read()\n","        faceCoordinates = fdu.getFaceCoordinates(frame)\n","        refreshFrame(frame, faceCoordinates)\n","        \n","        if faceCoordinates is not None:\n","            cnt -= 1\n","            face_img = fdu.preprocess(frame, faceCoordinates, face_shape=FACE_SHAPE)\n","            #cv2.imshow(windowsName, face_img)\n","\n","            input_img = np.expand_dims(face_img, axis=0)\n","            input_img = np.expand_dims(input_img, axis=0)\n","\n","            result = model.predict(input_img)[0]\n","            if cnt == 4:\n","                tot_result = result\n","            else:\n","                tot_result += result\n","            index = np.argmax(result)\n","            print ('Frame',5-cnt,':', emo[index], 'prob:', max(result))\n","            #index = np.argmax(result)\n","            #print (emo[index], 'prob:', max(result))\n","            # print(face_img.shape)\n","            # emotion = class_label[result_index]\n","            # print(emotion)\n","    index = np.argmax(tot_result)\n","    print ('Final decision:',emo[index], 'prob:', max(tot_result))\n","    return emo[index]\n","\n","def getCameraStreaming():\n","    capture = cv2.VideoCapture(0)\n","    if not capture:\n","        print(\"Failed to capture video streaming \")\n","        sys.exit(1)\n","    else:\n","        print(\"Successed to capture video streaming\")\n","        \n","    return capture\n","\n","def main():\n","    '''\n","    Arguments to be set:\n","        showCam : determine if show the camera preview screen.\n","    '''\n","    print(\"Enter main() function\")\n","    \n","    capture = getCameraStreaming()\n","\n","    cv2.startWindowThread()\n","    cv2.namedWindow(windowsName, cv2.WND_PROP_FULLSCREEN)\n","    cv2.setWindowProperty(windowsName, cv2.WND_PROP_FULLSCREEN, cv2.WND_PROP_FULLSCREEN)\n","    \n","    while True:\n","        recContent = speechRecognition()\n","        if recContent is not None:\n","            emotion = showScreenAndDectect(capture)\n","            if emotion == \"Angry\":\n","                emoji = \" >:O\"\n","            elif emotion == \"Fear\":\n","                emoji = \" :-S\"\n","            elif emotion == \"Happy\":\n","                emoji = \" :-D\"\n","            elif emotion == \"Sad\":\n","                emoji = \" :'(\"\n","            elif emotion == \"Surprise\":\n","                emoji = \" :-O\"\n","            else:\n","                emoji = \" \"\n","            print(\"Output result: \" + recContent + emoji)\n","\n","if __name__ == '__main__':\n","    main()\n"],"metadata":{"id":"7hSErXeBsIc8"},"execution_count":null,"outputs":[]}]}